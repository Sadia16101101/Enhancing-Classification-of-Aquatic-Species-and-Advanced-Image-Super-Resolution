{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q4TfFae_lL4",
        "outputId": "27f3e267-99f9-4f4d-8a78-f0e1933045f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_invasive = '/content/drive/MyDrive/aqua_data/invasive'\n",
        "path_to_non_invasive = '/content/drive/MyDrive/aqua_data/non_invasive'\n"
      ],
      "metadata": {
        "id": "Gy9VK1s0Ba6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def load_images_and_labels(image_paths, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    try:\n",
        "        for image_path in os.listdir(image_paths):\n",
        "            img = load_img(os.path.join(image_paths, image_path), target_size=(32, 32))\n",
        "            img = img_to_array(img)\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "x_invasive, y_invasive = load_images_and_labels(path_to_invasive, 1)\n",
        "x_non_invasive, y_non_invasive = load_images_and_labels(path_to_non_invasive, 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--_VnZ2RBwev",
        "outputId": "90b1de33-ea61-44e9-9010-ae64a54e26eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: cannot identify image file <_io.BytesIO object at 0x7db278f01580>\n",
            "Error: cannot identify image file <_io.BytesIO object at 0x7db1e42dd3f0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = np.concatenate((x_invasive, x_non_invasive), axis=0)\n",
        "y = np.concatenate((y_invasive, y_non_invasive), axis=0)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "i2h5qbPWH35F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n"
      ],
      "metadata": {
        "id": "dR1ghDW7H8LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)\n"
      ],
      "metadata": {
        "id": "L-AiAMV2H_m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMnRCVEWIHIa",
        "outputId": "5881d988-d2a1-49ff-d56c-6b377353f891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (1397, 32, 32, 3) - y_train shape: (1397, 2)\n",
            "x_test shape: (350, 32, 32, 3) - y_test shape: (350, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZAK5CuaIYGW",
        "outputId": "126310a4-5cf8-4a23-facf-37e7964a8656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/611.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/611.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZIMvhJgIUfK",
        "outputId": "9bfb82d9-9439-4046-d359-ca6dfcac2388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.02),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Setting the state of the normalization layer.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "metadata": {
        "id": "VGoQ2zUZINrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "input_shape = (32, 32, 3)"
      ],
      "metadata": {
        "id": "UBnEj1UmIm5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_encoder():\n",
        "    resnet = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
        "    )\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = resnet(augmented)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
        "    return model\n",
        "\n",
        "\n",
        "encoder = create_encoder()\n",
        "encoder.summary()\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 265\n",
        "hidden_units = 512\n",
        "projection_units = 128\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.5\n",
        "temperature = 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkHyPYvXIdjn",
        "outputId": "0f3350c0-a487-4d6e-b833-c105aedaee30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10-encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 32, 32, 3)         7         \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23564807 (89.89 MB)\n",
            "Trainable params: 23519360 (89.72 MB)\n",
            "Non-trainable params: 45447 (177.53 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.losses import CategoricalCrossentropy\n",
        "\n",
        "def create_classifier(encoder, trainable=True):\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=CategoricalCrossentropy(),  # Changed here\n",
        "        metrics=['accuracy']  # Simplified metric for clarity\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Create the encoder and classifier again\n",
        "encoder = create_encoder()\n",
        "classifier = create_classifier(encoder)\n",
        "\n",
        "# Now fit the model\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2TSmvacIv7p",
        "outputId": "bef2cfb1-d6e8-4e3b-f59a-088b3f477e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "6/6 [==============================] - 103s 13s/step - loss: 1.5838 - accuracy: 0.6664\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.2855 - accuracy: 0.8912\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 82s 13s/step - loss: 0.2004 - accuracy: 0.9477\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 86s 14s/step - loss: 0.1601 - accuracy: 0.9578\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.0751 - accuracy: 0.9742\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0700 - accuracy: 0.9764\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0464 - accuracy: 0.9800\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0497 - accuracy: 0.9800\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0343 - accuracy: 0.9893\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 86s 14s/step - loss: 0.0426 - accuracy: 0.9807\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.0297 - accuracy: 0.9864\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.0259 - accuracy: 0.9885\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.0148 - accuracy: 0.9950\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 82s 13s/step - loss: 0.0229 - accuracy: 0.9914\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 87s 14s/step - loss: 0.0223 - accuracy: 0.9921\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 86s 14s/step - loss: 0.0505 - accuracy: 0.9914\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.0275 - accuracy: 0.9893\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.0261 - accuracy: 0.9936\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 83s 14s/step - loss: 0.0181 - accuracy: 0.9936\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 88s 14s/step - loss: 0.0091 - accuracy: 0.9957\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0079 - accuracy: 0.9964\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0034 - accuracy: 0.9986\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 87s 14s/step - loss: 0.0397 - accuracy: 0.9893\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0452 - accuracy: 0.9850\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0161 - accuracy: 0.9943\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 82s 13s/step - loss: 0.0236 - accuracy: 0.9943\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.0216 - accuracy: 0.9964\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0139 - accuracy: 0.9957\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 83s 14s/step - loss: 0.0060 - accuracy: 0.9971\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0024 - accuracy: 0.9993\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 86s 14s/step - loss: 0.0026 - accuracy: 0.9993\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0149 - accuracy: 0.9950\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0431 - accuracy: 0.9907\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0296 - accuracy: 0.9921\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 81s 13s/step - loss: 0.0232 - accuracy: 0.9900\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0162 - accuracy: 0.9928\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0105 - accuracy: 0.9964\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0089 - accuracy: 0.9993\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 81s 13s/step - loss: 0.0039 - accuracy: 0.9986\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0056 - accuracy: 0.9979\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 81s 13s/step - loss: 0.0063 - accuracy: 0.9986\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 83s 13s/step - loss: 0.0021 - accuracy: 0.9986\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 84s 13s/step - loss: 0.0012 - accuracy: 0.9993\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 1.8916e-04 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0039 - accuracy: 0.9986\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0105 - accuracy: 0.9979\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 84s 14s/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 85s 14s/step - loss: 0.0031 - accuracy: 0.9993\n",
            "11/11 [==============================] - 4s 199ms/step - loss: 0.1099 - accuracy: 0.9629\n",
            "Test accuracy: 96.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x78fOn6gOkxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRxAPDLUOk-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
        "\n",
        "\n",
        "def add_projection_head(encoder):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=inputs, outputs=outputs, name=\"with_projection-head\"\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "KN2UGwOhI4Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = create_encoder()\n",
        "\n",
        "encoder_with_projection_head = add_projection_head(encoder)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature),\n",
        ")\n",
        "\n",
        "encoder_with_projection_head.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg_t9GEmM36A",
        "outputId": "af608251-674d-4bf3-edbc-2715c5f21287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"with_projection-head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_23 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functiona  (None, 2048)              23564807  \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               262272    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23827079 (90.89 MB)\n",
            "Trainable params: 23781632 (90.72 MB)\n",
            "Non-trainable params: 45447 (177.53 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = encoder_with_projection_head.fit(\n",
        "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "y8M_xOGtM7oN",
        "outputId": "f9af4303-5325-45fc-adde-9dc6ec54c710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node softmax_cross_entropy_with_logits_1 defined at (most recent call last):\n<stack traces unavailable>\nlogits and labels must be broadcastable: logits_size=[265,265] labels_size=[530,265]\n\t [[{{node softmax_cross_entropy_with_logits_1}}]] [Op:__inference_train_function_176764]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-840009061bbf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = encoder_with_projection_head.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node softmax_cross_entropy_with_logits_1 defined at (most recent call last):\n<stack traces unavailable>\nlogits and labels must be broadcastable: logits_size=[265,265] labels_size=[530,265]\n\t [[{{node softmax_cross_entropy_with_logits_1}}]] [Op:__inference_train_function_176764]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = create_classifier(encoder, trainable=False)\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "i_7v3ouDc51z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "87"
      ],
      "metadata": {
        "id": "PsHx-784cXEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Test accuracy: 89.43%\\n\""
      ],
      "metadata": {
        "id": "VO_xA9MbcYSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}