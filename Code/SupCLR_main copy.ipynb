{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c365ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d127721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c8e2f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: torch-1.0.1-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\n",
      "Requirement already satisfied: numpy in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Collecting torch==2.0.1\n",
      "  Using cached torch-2.0.1-cp310-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: requests in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from torchvision) (2.29.0)\n",
      "Requirement already satisfied: jinja2 in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (1.12)\n",
      "Requirement already satisfied: filelock in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
      "Requirement already satisfied: networkx in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sadianasrintisha/anaconda3/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-2.0.1 torchvision-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-win_amd64.whl\n",
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74b83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e9ff6",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669cd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn1='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/noninvasive/*/'\n",
    "trn2='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracods1/*/'\n",
    "trn3='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/invasive/*/'\n",
    "trn31='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracod vs Pedi-Veliger Examples-selected/Preserved Zebra Ped 1a To Baylor/Sorted Images/Preserved Zebra Ped 1a/*/'\n",
    "trn11='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracod vs Pedi-Veliger Examples-selected/Preserved Zebra Ped 1a To Baylor/Sorted Images/Not/*/'\n",
    "trn12='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/To Baylor 2023-01-30/Not Veligers/O1/*/'\n",
    "trn13='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/To Baylor 2023-01-30/Not Veligers/Z-P/*/'\n",
    "trn32='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/To Baylor 2023-01-30/Zebra Pediveliger Image1a/Zebra Pediveligers/*/'\n",
    "trn14='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/USGS Labled Zebra-selected/Preserved Zebra D-Hinge 1 Baylor/Sorted Images/Not/*/'\n",
    "trn21='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/USGS Labled Zebra-selected/Preserved Zebra D-Hinge 1 Baylor/Sorted Images/Ostracod/*/'\n",
    "trn33='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/USGS Labled Zebra-selected/Preserved Zebra D-Hinge 1 Baylor/Sorted Images/Zebra D-Hinge/*/'\n",
    "\n",
    "# trn15='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/NonVeligers/*/'\n",
    "# trn34='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/Veligers/*/'\n",
    "trn16='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Baylor Lake Minnetonka Chk4 Image1/Not-Invasive Images/*/'\n",
    "trn35='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Davis Dam 2022-7-25/Imaging Set 1/Labeled Data/Veligers/*/'\n",
    "trn17='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Davis Dam 2022-7-25/Imaging Set 1/Labeled Data/Not-Veligers/*/'\n",
    "trn36='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Davis Dam 2022-7-25/Imaging Set 2/Label Data/Veligers/*/'\n",
    "trn18='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Davis Dam 2022-7-25/Imaging Set 2/Label Data/Not-Veligers/*/'\n",
    "trn22='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracod Day 2 Image12 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn19='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracod Day 2 Image12 To Baylor/Sorted Images/Not/*/'\n",
    "trn23='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracods Day 2 Image1 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn110='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracods Day 2 Image1 To Baylor/Sorted Images/Not/*/'\n",
    "trn24='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracods Day 2 Image2 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn111='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracods Day 2 Image2 To Baylor/Sorted Images/Not/*/'\n",
    "trn25='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracods Day 2 Image3 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn112='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracods Day 2 Image3 To Baylor/Sorted Images/Not/*/'\n",
    "trn26='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracods Day 2 Image12 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn113='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Veliger Data-selected/Ostracods Day 2 Image12 To Baylor/Sorted Images/Not/*/'\n",
    "trn37='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracod vs Pedi-Veliger Examples-selected/Preserved Zebra Ped 1 To Baylor/Sorted Images/Pedi-Zebra Veligers/*/'\n",
    "trn114='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracod vs Pedi-Veliger Examples-selected/Preserved Zebra Ped 1 To Baylor/Sorted Images/Not/*/'\n",
    "trn27='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracod vs Pedi-Veliger Examples-selected/Preserved Ostracods 1a To Baylor/Sorted Images/Preserved Ostracods 1a/*/'\n",
    "trn114='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracod vs Pedi-Veliger Examples-selected/Preserved Ostracods 1a To Baylor/Sorted Images/Not/*/'\n",
    "trn28='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracod vs Pedi-Veliger Examples-selected/Preserved Ostracods 1 To Baylor/Sorted Images/Preserve Ostracods/*/'\n",
    "trn115='/Users/sadianasrintisha/Desktop/Masters/Code/Dataset/Ostracod vs Pedi-Veliger Examples-selected/Preserved Ostracods 1 To Baylor/Sorted Images/Not/*/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c744666",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn1= glob(trn1)\n",
    "trn2= glob(trn2)\n",
    "trn3= glob(trn3)\n",
    "trn31= glob(trn31)\n",
    "trn32= glob(trn32)\n",
    "trn33= glob(trn33)\n",
    "# trn34= glob(trn34)\n",
    "trn35= glob(trn35)\n",
    "trn36= glob(trn36)\n",
    "trn37= glob(trn37)\n",
    "trn11= glob(trn11)\n",
    "trn12= glob(trn12)\n",
    "trn13= glob(trn13)\n",
    "trn14= glob(trn14)\n",
    "# trn15= glob(trn15)\n",
    "trn16= glob(trn16)\n",
    "trn17= glob(trn17)\n",
    "trn18= glob(trn18)\n",
    "trn19= glob(trn19)\n",
    "trn110= glob(trn110)\n",
    "trn111= glob(trn111)\n",
    "trn112= glob(trn112)\n",
    "trn113= glob(trn113)\n",
    "trn114= glob(trn114)\n",
    "trn115= glob(trn115)\n",
    "trn21= glob(trn21)\n",
    "trn22= glob(trn22)\n",
    "trn23= glob(trn23)\n",
    "trn24= glob(trn24)\n",
    "trn25= glob(trn25)\n",
    "trn26= glob(trn26)\n",
    "trn27= glob(trn27)\n",
    "trn28= glob(trn28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4993c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn1.extend(trn11)\n",
    "trn1.extend(trn12)\n",
    "trn1.extend(trn13)\n",
    "trn1.extend(trn14)\n",
    "# trn1.extend(trn15)\n",
    "trn1.extend(trn16)\n",
    "trn1.extend(trn17)\n",
    "trn1.extend(trn18)\n",
    "trn1.extend(trn19)\n",
    "trn1.extend(trn110)\n",
    "trn1.extend(trn111)\n",
    "trn1.extend(trn112)\n",
    "trn1.extend(trn113)\n",
    "trn1.extend(trn114)\n",
    "trn1.extend(trn115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da864ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn2.extend(trn21)\n",
    "trn2.extend(trn22)\n",
    "trn2.extend(trn23)\n",
    "trn2.extend(trn24)\n",
    "trn2.extend(trn25)\n",
    "trn2.extend(trn26)\n",
    "trn2.extend(trn27)\n",
    "trn2.extend(trn28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7a35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn3.extend(trn31)\n",
    "trn3.extend(trn32)\n",
    "trn3.extend(trn33)\n",
    "# trn3.extend(trn34)\n",
    "trn3.extend(trn35)\n",
    "trn3.extend(trn36)\n",
    "trn3.extend(trn37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f04a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1= shuffle(trn1)\n",
    "tr2= shuffle(trn2)\n",
    "tr3= shuffle(trn3)\n",
    "\n",
    "tran_index_noninv = np.round( len(tr1)* .6  )\n",
    "tran_index_osc = np.round( len(tr2)* .6  )\n",
    "tran_index_inv = np.round( len(tr3)* .6  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "860fe142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_index_noninv\n",
    "tran_index_inv\n",
    "tran_index_osc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3225bd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Objects for Non-Invasive, Invasive and Ostracod : 5685 1220 104\n"
     ]
    }
   ],
   "source": [
    "print ( \"Number of Objects for Non-Invasive, Invasive and Ostracod :\" , len(tr1), len(tr3), len(tr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cbf7b",
   "metadata": {},
   "source": [
    "# Total 7009"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492176b",
   "metadata": {},
   "source": [
    "# Traindata read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8c7cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr3[:(int) (tran_index_inv)]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr1[:(int) (tran_index_noninv)]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr2[:(int) (tran_index_osc)]:\n",
    "    label.append(2)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in range(0,len(tr3[:(int) (tran_index_inv)])):\n",
    "    a = glob(tr3[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr1[:(int) (tran_index_noninv)])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])        \n",
    "\n",
    "for j in range(0,len(tr2[:(int) (tran_index_osc)])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k]) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b49ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a1c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135698\n"
     ]
    }
   ],
   "source": [
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f12594fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 32, 32)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c389f18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21025, 32, 32, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_train = idata\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5e0be99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21025, 32, 32, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca5cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21025, 32, 32, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (len(X_train),32,32,3))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f6e7092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21025, 32, 32, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "843a6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end= 0\n",
    "# train_df= []\n",
    "# breath = 5\n",
    "\n",
    "# i = 0\n",
    "# for i in range(0, len(label)):\n",
    "#     deff = []\n",
    "#     for k in range(0, (breath)):\n",
    "        \n",
    "#         index = (i+k)\n",
    "        \n",
    "#         deff.append(X_train[index])\n",
    "        \n",
    "#     train_df.append(deff)\n",
    "\n",
    "# Y_train = to_categorical(label)\n",
    "# train_df = np.array(train_df)\n",
    "# YY_Train = label\n",
    "# np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f20b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end= 0\n",
    "# train_df= []\n",
    "# breath = 5\n",
    "\n",
    "# i = 0\n",
    "# for i in range(0, len(label)):\n",
    "#     deff = []\n",
    "#     for k in range(0, (breath)):\n",
    "        \n",
    "#         index = (i*5+k)\n",
    "        \n",
    "#         deff.append(X_train[index])\n",
    "        \n",
    "#     train_df.append(deff)\n",
    "\n",
    "# Y_train = to_categorical(label)\n",
    "# train_df = np.array(train_df)\n",
    "# YY_Train = label\n",
    "# np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e477726",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(label)\n",
    "YY_Train = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3cadd",
   "metadata": {},
   "source": [
    "# Train Data 4205\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa105845",
   "metadata": {},
   "source": [
    "# Test data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de438d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr3[(int) (tran_index_inv) + 1 :]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr1[ (int)(tran_index_noninv) + 1:]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in tr2[ (int)(tran_index_osc) + 1:]:\n",
    "    label.append(2)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in range(0,len(tr3[(int) (tran_index_inv) + 1 :])):\n",
    "    a = glob(tr3[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr1[ (int)(tran_index_noninv) + 1:])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])  \n",
    "        \n",
    "for j in range(0,len(tr2[ (int)(tran_index_osc) + 1:])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25140be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 32, 32)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e281e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_test = idata\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_test = np.reshape(X_test, (len(X_test),32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27814aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14005, 32, 32, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da9e449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end= 0\n",
    "# test_df= []\n",
    "# breath = 5\n",
    "\n",
    "# i = 0\n",
    "# for i in range(0, len(label)):\n",
    "#     deff = []\n",
    "#     for k in range(0, (breath)):\n",
    "        \n",
    "#         index = (i*5 + k)\n",
    "        \n",
    "#         deff.append(X_test[index])\n",
    "        \n",
    "#     test_df.append(deff)\n",
    "    \n",
    "# Y_test = to_categorical(label)\n",
    "# test_df = np.array(test_df)\n",
    "# YY_Test = label\n",
    "# np.shape(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd4826af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = to_categorical(label)\n",
    "YY_Test = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade76dd",
   "metadata": {},
   "source": [
    "# Test data 2801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f21a9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4205, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "random_indices = np.random.choice(X_train.shape[0], Y_train.shape[0], replace=False)\n",
    "\n",
    "# Select the subset of X_train based on the random indices\n",
    "X_train_resized = X_train[random_indices]\n",
    "\n",
    "# Check the new shape\n",
    "print(X_train_resized.shape)  # (2801, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f5b2813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2801, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "random_indices = np.random.choice(X_test.shape[0], Y_test.shape[0], replace=False)\n",
    "\n",
    "# Select the subset of X_train based on the random indices\n",
    "X_test_resized = X_test[random_indices]\n",
    "\n",
    "# Check the new shape\n",
    "print(X_test_resized.shape)  # (2801, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3af4b4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4205, 1)\n",
      "(2801, 1)\n"
     ]
    }
   ],
   "source": [
    "# Select the first column of Y_train and reshape it\n",
    "Y_train_resized = np.reshape(Y_train[:, 0], (-1, 1))\n",
    "\n",
    "# Select the first column of Y_test and reshape it\n",
    "Y_test_resized = np.reshape(Y_test[:, 0], (-1, 1))\n",
    "\n",
    "# Check the new shapes\n",
    "print(Y_train_resized.shape)\n",
    "print(Y_test_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a9ed5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train data shape: (4205, 32, 32, 3)\n",
      "X_Test data shape: (2801, 32, 32, 3)\n",
      "Y_Train data shape: (4205, 1)\n",
      "Y_Test data shape: (2801, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"X_Train data shape:\", X_train_resized.shape)\n",
    "print(\"X_Test data shape:\", X_test_resized.shape)\n",
    "print(\"Y_Train data shape:\", Y_train_resized.shape)\n",
    "print(\"Y_Test data shape:\", Y_test_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7053e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= X_train_resized\n",
    "x_test=X_test_resized\n",
    "y_train=Y_train_resized\n",
    "y_test=Y_test_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f472602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train data shape: (4205, 32, 32, 3)\n",
      "X_Test data shape: (2801, 32, 32, 3)\n",
      "Y_Train data shape: (4205, 1)\n",
      "Y_Test data shape: (2801, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_Train data shape:\", x_train.shape)\n",
    "print(\"X_Test data shape:\", x_test.shape)\n",
    "print(\"Y_Train data shape:\", y_train.shape)\n",
    "print(\"Y_Test data shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7fd440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "input_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98cf5de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 11:13:02.812219: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.02),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53506d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"invasive_noninvasive_ostracod\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (4,)                0           ['input_2[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 32, 32, 3)    0           ['input_2[0][0]',                \n",
      "                                                                  'tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " random_flip_1 (RandomFlip)     (None, 32, 32, 3)    0           ['tf.reshape[0][0]']             \n",
      "                                                                                                  \n",
      " resnet50v2 (Functional)        (None, 2048)         23564800    ['random_flip_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,564,800\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_encoder():\n",
    "    input_shape = (32, 32, 3)\n",
    "    resnet = keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
    "    )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = tf.keras.backend.shape(inputs)\n",
    "    augmented = tf.reshape(inputs, (augmented[0],) + input_shape)\n",
    "    augmented = keras.layers.experimental.preprocessing.RandomFlip()(augmented)\n",
    "    outputs = resnet(augmented)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"invasive_noninvasive_ostracod\")\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 265\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4face36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"invasive\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57bd6833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"invasive\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " invasive_noninvasive_ostrac  (None, 2048)             23564800  \n",
      " od (Functional)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,615,427\n",
      "Trainable params: 24,569,987\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 34s 2s/step - loss: 0.9855 - sparse_categorical_accuracy: 0.7344\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.6034 - sparse_categorical_accuracy: 0.8024\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.5247 - sparse_categorical_accuracy: 0.8093\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.5319 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.5428 - sparse_categorical_accuracy: 0.8081\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.5186 - sparse_categorical_accuracy: 0.8076\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 0.5212 - sparse_categorical_accuracy: 0.8105\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.5387 - sparse_categorical_accuracy: 0.8036\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.5031 - sparse_categorical_accuracy: 0.8093\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.4694 - sparse_categorical_accuracy: 0.8124\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 0.4400 - sparse_categorical_accuracy: 0.8271\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 0.3993 - sparse_categorical_accuracy: 0.8373\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 0.3397 - sparse_categorical_accuracy: 0.8694\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 0.3457 - sparse_categorical_accuracy: 0.8578\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 0.2927 - sparse_categorical_accuracy: 0.8839\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 0.2898 - sparse_categorical_accuracy: 0.8859\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.2672 - sparse_categorical_accuracy: 0.8985\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.2658 - sparse_categorical_accuracy: 0.8923\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.2534 - sparse_categorical_accuracy: 0.9011\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.2420 - sparse_categorical_accuracy: 0.9080\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.2364 - sparse_categorical_accuracy: 0.9141\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.2403 - sparse_categorical_accuracy: 0.9061\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.2398 - sparse_categorical_accuracy: 0.9013\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 34s 2s/step - loss: 0.2160 - sparse_categorical_accuracy: 0.9130\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 35s 2s/step - loss: 0.1767 - sparse_categorical_accuracy: 0.9301\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 34s 2s/step - loss: 0.1863 - sparse_categorical_accuracy: 0.9284\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.1565 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.1171 - sparse_categorical_accuracy: 0.9539\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.1098 - sparse_categorical_accuracy: 0.9596\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.1525 - sparse_categorical_accuracy: 0.9405\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.1196 - sparse_categorical_accuracy: 0.9532\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.0949 - sparse_categorical_accuracy: 0.9622\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.1030 - sparse_categorical_accuracy: 0.9608\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.1074 - sparse_categorical_accuracy: 0.9555\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9650\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.1329 - sparse_categorical_accuracy: 0.9501\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.1000 - sparse_categorical_accuracy: 0.9636\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.1063 - sparse_categorical_accuracy: 0.9603\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0886 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0706 - sparse_categorical_accuracy: 0.9731\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0916 - sparse_categorical_accuracy: 0.9620\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0729 - sparse_categorical_accuracy: 0.9741\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0807 - sparse_categorical_accuracy: 0.9710\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0726 - sparse_categorical_accuracy: 0.9753\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0671 - sparse_categorical_accuracy: 0.9743\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0656 - sparse_categorical_accuracy: 0.9760\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0395 - sparse_categorical_accuracy: 0.9857\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0712 - sparse_categorical_accuracy: 0.9727\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0875 - sparse_categorical_accuracy: 0.9693\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9838\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 1.6342 - sparse_categorical_accuracy: 0.7387\n",
      "Test accuracy: 73.87%\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "classifier = create_classifier(encoder)\n",
    "classifier.summary()\n",
    "\n",
    "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a096c5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 3s 30ms/step - loss: 1.6342 - sparse_categorical_accuracy: 0.7387\n",
      "Test accuracy: 73.87%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f59f221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"with_projection-head\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3fdb509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"with_projection-head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " invasive_noninvasive_ostrac  (None, 2048)             23564800  \n",
      " od (Functional)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,827,072\n",
      "Trainable params: 23,781,632\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01441e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 5.9200\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.5721\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.5718\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.5717\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.5714\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.5702\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.5694\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 5.5709\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 36s 2s/step - loss: 5.5650\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 37s 2s/step - loss: 5.5582\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 35s 2s/step - loss: 5.5510\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 5.5676\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5510\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5505\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5358\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5379\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5214\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5205\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5519\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5161\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5086\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.5240\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 34s 2s/step - loss: 5.5178\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 5.4834\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 33s 2s/step - loss: 5.5419\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.4858\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.4830\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.4735\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 5.4582\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 448s 30s/step - loss: 5.4987\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 31s 2s/step - loss: 5.4424\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 5.4781\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 5.4509\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 5.4631\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 5.4172\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.4426\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.4080\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.4305\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.3671\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.4540\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.3429\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.4159\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.3782\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 29s 2s/step - loss: 5.3829\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.3394\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.3553\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.3169\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.3526\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.3381\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 5.2921\n"
     ]
    }
   ],
   "source": [
    "history = encoder_with_projection_head.fit(\n",
    "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ac3954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 4s 147ms/step - loss: 3.8520 - sparse_categorical_accuracy: 0.7172\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 2.0279 - sparse_categorical_accuracy: 0.7529\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 1.3209 - sparse_categorical_accuracy: 0.7555\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.8676 - sparse_categorical_accuracy: 0.7769\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.6168 - sparse_categorical_accuracy: 0.7850\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5563 - sparse_categorical_accuracy: 0.7990\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.5283 - sparse_categorical_accuracy: 0.8059\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5277 - sparse_categorical_accuracy: 0.8081\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5231 - sparse_categorical_accuracy: 0.8069\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.5153 - sparse_categorical_accuracy: 0.8088\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 0.5124 - sparse_categorical_accuracy: 0.8095\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5124 - sparse_categorical_accuracy: 0.8088\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5154 - sparse_categorical_accuracy: 0.8093\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5086 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5061 - sparse_categorical_accuracy: 0.8102\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5042 - sparse_categorical_accuracy: 0.8109\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.5046 - sparse_categorical_accuracy: 0.8107\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.4999 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5062 - sparse_categorical_accuracy: 0.8100\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.5031 - sparse_categorical_accuracy: 0.8105\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.5051 - sparse_categorical_accuracy: 0.8105\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5012 - sparse_categorical_accuracy: 0.8098\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.5029 - sparse_categorical_accuracy: 0.8105\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.4985 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.5002 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.4980 - sparse_categorical_accuracy: 0.8117\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.4949 - sparse_categorical_accuracy: 0.8109\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4968 - sparse_categorical_accuracy: 0.8114\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4947 - sparse_categorical_accuracy: 0.8109\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.4964 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.4957 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4940 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4964 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.4937 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.4965 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.4925 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.4942 - sparse_categorical_accuracy: 0.8109\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.4927 - sparse_categorical_accuracy: 0.8114\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.4940 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.4938 - sparse_categorical_accuracy: 0.8109\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.4970 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 3s 156ms/step - loss: 0.4939 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.4941 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.4913 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.4951 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.4933 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.4896 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.4906 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.4895 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.4943 - sparse_categorical_accuracy: 0.8112\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 0.4876 - sparse_categorical_accuracy: 0.8115\n",
      "Test accuracy: 81.15%\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier(encoder, trainable=False)\n",
    "\n",
    "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "363c9a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 3s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.3178628e-01, 7.6821369e-01, 2.4778897e-08],\n",
       "       [1.8932779e-01, 8.1066906e-01, 3.2133444e-06],\n",
       "       [1.8627964e-01, 8.1371748e-01, 2.8406157e-06],\n",
       "       ...,\n",
       "       [1.6950706e-01, 8.3049202e-01, 9.1765725e-07],\n",
       "       [2.3033144e-01, 7.6966470e-01, 3.8517410e-06],\n",
       "       [1.8856429e-01, 8.1143194e-01, 3.7396089e-06]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c15e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.round(pred)\n",
    "p = np.argmax ( p , axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc94c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.15%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, p)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa549b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
